{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b712733-71ab-4d52-85ca-f76d65a45f3f",
   "metadata": {
    "id": "6b712733-71ab-4d52-85ca-f76d65a45f3f"
   },
   "source": [
    "# BEFORE YOU START\n",
    "- Below is a template of the notebook in which demonstrate your solution.\n",
    "- The notebook should be a stand-alone entity, meaning that anyone with basic CS and AI / ML knowledge who reads it should be able to understand your problem.\n",
    "- The notebook must be in regular notebook format, e.g., you may not use RISE.\n",
    "- This template is a guideline, and you may make any changes required to best present your problem and solution. **HOWEVER**, you must include all the information that is required of you in this template. Read the text and comments in the different sections to find out what these requirements are.\n",
    "- Cite / link your sources, including all papers, blogs, and images taken from the web.\n",
    "- Sections that require code have code cells with coding todos for that section.\n",
    "- You are not required to stick to the placement of the code. It is preferred to sprinkle many short code cells within textual context than to put all code together at the end of the section (much more readable)\n",
    "- Remember to change the project title and author lines!\n",
    "- Do not include this section in your final submission (delte this cell)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb93415-955d-4398-9636-93f4ceaa05bb",
   "metadata": {
    "id": "bcb93415-955d-4398-9636-93f4ceaa05bb"
   },
   "source": [
    "# Settelers of catan inspired by Alphazero approach\n",
    "Aviv Cohen; avivcohen@campus.technion.ac.il\n",
    "\n",
    "Dan Navon; danavon@campus.technion.ac.il"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4084f25-7893-45d9-9522-9b134f38878e",
   "metadata": {
    "id": "a4084f25-7893-45d9-9522-9b134f38878e"
   },
   "source": [
    "## Introduction\n",
    "\n",
    "An introduction should include the following:\n",
    "- An informal description of the problem you are trying to solve (examples are best)\n",
    "- Motivation for solving this problem\n",
    "- An explanation why it is hard to solve this problem\n",
    "- Previous methods for solving the problem and their strengths and weaknesses\n",
    "- A description of your solution to the problem, how it overcomes the issues in previous mehtods, and what new issues arise.\n",
    "- A short description on how you intend to evaluate your solution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6524437f",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Previous methods:\n",
    "The ‘world champion’ standard of AI for this board game is a program called JSettlers, developed by Jeremy Monin over 10 years ago and still in active development today. This program is developed in Java and contains a fully-implemented framework of the board game as well as an entirely rule-based agent with hard-coded decisions and heuristics.\n",
    "\n",
    "Previous research was endeavored by the QSettlers team led by Peter McAughan in 2019. In this work, the team attempted to similarly apply the DQN paradigm to develop an AI model to play and win Settlers of Catan.\n",
    "Unfortunately, the team was unsuccessful at implementing a working DQN model for general gameplay, but was able to train and develop a working DQN model specifically for the player trading mechanism of the game.\n",
    "        \n",
    "At [Re-L Catan: Evaluation of Deep Reinforcement Learning for\n",
    "Resource Management Under Competitive and Uncertain\n",
    "Environments](https://cs230.stanford.edu/projects_fall_2021/reports/103176936.pdf) the authors uses Deep Q-Learning on different part of the game in order to the determine if this is the right action to do (e.g. building/trading) but they ignore the whole gameplay.\n",
    "\n",
    "#TODO:\n",
    "Game strategis \n",
    "\n",
    "[Optimizing UCT for Settlers of Catan](https://www.sbgames.org/sbgames2017/papers/ComputacaoFull/175405.pdf) based on [Monte-Carlo Tree Search in Settlers of Catan](https://www.researchgate.net/publication/220716999_Monte-Carlo_Tree_Search_in_Settlers_of_Catan)\n",
    "\n",
    "[Mastering the game of Go with deep neural networks and tree search](https://www.nature.com/articles/nature16961.pdf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e00ac8c3",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# TODO: provide a step-by-step guide to running your domain in code\n",
    "#  - initialize an environment instance\n",
    "#  - visualize observations ()\n",
    "#  - show how to take an action.\n",
    "#  - play a game / an episode / several timesteps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21378ca6",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Domain - Catan\n",
    "\n",
    "We forked our project from [PyCatan2](https://github.com/josefwaller/PyCatan2), which gives a raw implmentation of the game in order to let other developers to implment the rules they inted to use.\n",
    "In our case, we implmented the very basic game elements - building cities, settelments, roads and trading (no players' trading).\n",
    "The reason is to keep the game state fully visible as in other AlphaZero games implementation (Go, Chess, 4-in a row, tic-tac-toe and etc) so we can be compatible with the original game.\n",
    "\n",
    "Full explanation of the game can be found [here](https://en.wikipedia.org/wiki/Catan) or under docs directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8e1058e",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os.path\n",
    "import random\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from src.mcts import mcts_get_best_action\n",
    "from src.mlp import MLP\n",
    "from src.dataset import Dataset\n",
    "from src.training import MLPTrainer\n",
    "from src.plot import plot_fit\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f36df8",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We wrapped the PyCatan2 in order to let the agents interact with game.\n",
    "\n",
    "One can change <code>catan_wrp.py</code> , add or remove different rules of the game, change the initial board and etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2857c2c",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Board:\n",
      "                                                       \n",
      "                                                       \n",
      "                                                       \n",
      "                                                       \n",
      "                 3:1         2:1                       \n",
      "                  .--'--.--'--.--'--.                  \n",
      "                  | 10  |  2  |  9  | 2:1              \n",
      "               .--'--.--'--.--'--.--'--.               \n",
      "           2:1 | 12  |  6  |  4  | 10  |               \n",
      "            .--'--.--'--.--'--.--'--.--'--.            \n",
      "            |  9  | 11  |   R |  3  |  8  | 3:1        \n",
      "            '--.--'--.--'--.--'--.--'--.--'            \n",
      "           2:1 |  8  |  3  |  4  |  5  |               \n",
      "               '--.--'--.--'--.--'--.--'               \n",
      "                  |  5  |  6  | 11  | 2:1              \n",
      "                  '--.--'--.--'--.--'                  \n",
      "                 3:1         3:1                       \n",
      "                                                       \n",
      "                                                       \n",
      "                                                       \n"
     ]
    }
   ],
   "source": [
    "from src.catan_wrp import Catan\n",
    "num_players = 4\n",
    "catan_game = Catan()\n",
    "print(\"Board:\")\n",
    "print(catan_game.game.board)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "596f4e68",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The coordination system called is a skwed 2D grid, more details can be found under <code>/docs/Working-with-Board.srt</code>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b1258a",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<img src=\"docs/images/catangrid_withpieces.png\" align=\"left\"/>\n",
    "<img src=\"BeginnerBoard.jpg\" align=\"right\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ffd105a",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Observations\n",
    "The observations returned as a seralized vector.\n",
    "The reason for this serialization is that we would like to predict how good a state is using a DNN which have to recive a constanst size input vector.\n",
    "\n",
    "observations represent = <code>[current_player, dice, longest_road, initializtion_stage, intersection_buildings, roads, resources, harbors] </code>.\n",
    "1. <code>current_player</code> - current player id.\n",
    "2. <code>dice</code> - dice value can be ranged between 2-12.\n",
    "3. <code>longest_road</code> - the player id who owns the longest road (2 victory points).\n",
    "4. <code>initializtion_stage</code> - boolean value, indicates if we are at the initialization stage.\n",
    "5. <code>intersection_buildings</code> - vector with 55 elements where values represents the 55 intersections along the board. values repesented as $player\\_id*10 + building\\_type$ where $building\\_type$ may be- 1-Settlement, 2-City. if no bulding han the value is - 0 - No building.\n",
    "for example the value \"12\" means that the intersection belongs to player_id==int(12/10)==1 with a city(12%10==2) on it.\n",
    "6. <code>roads</code> - vector with 71 elements - represnts all posiible locations to place roads at. values may be - 0 - No road, otherwise - player_id+1 where the player_id represnt who onws this road.\n",
    "7. <code>resources</code> - there are 5 different types of resources (lumber, brick, ore, grain, grass), assuming 4 players in the game, this vector will have 20 elments where each elments represnts the amount of each resource\n",
    "8. <code>harbors</code> - 9 elements, the values are as in intersection_buildings, harbors change the trading rates from 4/1 to 3/1\n",
    " \n",
    "overall the state is reprensted in a compcat 160 essential elements.\n",
    "\n",
    "at this image, green dots represnting the intersection, between each 2 adjacent dots, a road can be placed\n",
    "\n",
    "<img src=\"intersections.png\"/>\n",
    "\n",
    "Image taken from [\"RE-L Catan\"](https://cs230.stanford.edu/projects_fall_2021/reports/103176936.pdf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2dad7186",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state size:160\n",
      "tensor([0., 8., 0., 0., 2., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n"
     ]
    }
   ],
   "source": [
    "state = catan_game.get_state()\n",
    "print(\"state size:\" + str(len(state)))\n",
    "print(state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1583eb24",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Actions\n",
    "The actions space contains - building(city / settelement/ road), trading or end turn(+dice roll)\n",
    "Not all the action always avilable, the building and trading avilabilty depends on the number of resources each player has\n",
    "\n",
    "The first 8 turns (assuming 4 players) are being played in a different way.\n",
    "The order of the players is 1-2-3-4-4-3-2-1.\n",
    "This stage is the picking stage where each player pick a settlement and a road. at the reverse order (4-3-2-1) the players recive resources as well according to the surround hexes. \n",
    "Becuase each player can pick any leagal settlement at the beggining, the number of actions can be up to 55.\n",
    "\n",
    "Each action is represented as a tuple where the first element reflects the type of action and the second the coordinaton of the action.\n",
    "\n",
    "0. build a road\n",
    "1. build a settlement\n",
    "2. build a city\n",
    "3. trade resources (second value is the type of the trade)\n",
    "4. end turn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3b92693a",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<BuildingType.ROAD: 0>, frozenset({(q: 0, r:-4), (q: -1, r:-3)}))\n",
      "(<BuildingType.ROAD: 0>, frozenset({(q: -2, r:0), (q: -2, r:-1)}))\n",
      "(<BuildingType.ROAD: 0>, frozenset({(q: 0, r:-1), (q: 0, r:-2)}))\n",
      "(<BuildingType.ROAD: 0>, frozenset({(q: -3, r:2), (q: -4, r:3)}))\n",
      "(<BuildingType.ROAD: 0>, frozenset({(q: 1, r:-3), (q: 0, r:-2)}))\n",
      "(<BuildingType.ROAD: 0>, frozenset({(q: -5, r:2), (q: -5, r:3)}))\n",
      "(<BuildingType.ROAD: 0>, frozenset({(q: -3, r:-1), (q: -2, r:-1)}))\n",
      "(<BuildingType.ROAD: 0>, frozenset({(q: -4, r:4), (q: -4, r:3)}))\n",
      "(4,)\n",
      "                                                       \n",
      "                                                       \n",
      "                                                       \n",
      "                                                       \n",
      "                 3:1         2:1                       \n",
      "                  .--'--.--'--.--'--.                  \n",
      "                  | 10  |  2  |  9  | 2:1              \n",
      "               s--'--.--'--.--'--.--'--.               \n",
      "           2:1 | 12  |  6  |  4  | 10  |               \n",
      "            .--'--.--'--.--'--s--'--s--'--.            \n",
      "            |  9  | 11  |   R |  3  |  8  | 3:1        \n",
      "            '--.--'--.--'--.--'--.--'--.--c            \n",
      "           2:1 |  8  |  3  |  4  |  5  |               \n",
      "               '--.--s--.--s--.--'--.--'               \n",
      "                  |  5  |  6  | 11  | 2:1              \n",
      "                  s--.--'--.--'--.--s                  \n",
      "                 3:1         3:1                       \n",
      "                                                       \n",
      "                                                       \n",
      "                                                       \n"
     ]
    }
   ],
   "source": [
    "actions = catan_game.get_actions(prune=False)\n",
    "for a in actions:\n",
    "    print(a)\n",
    "best_action = random.choice(actions)\n",
    "catan_game.make_action(best_action)\n",
    "print(catan_game.game.board)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd05802",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Game simulation\n",
    "Here are a few moves to adress how the game is progressing.\n",
    "One can use the <code>src/text_game.py</code> in order to play by himself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b0d0e3",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for i in range(40):\n",
    "    actions = catan_game.get_actions(prune=False)\n",
    "    action = random.choice(actions)\n",
    "    catan_game.make_action(action)\n",
    "    print(\"Player \" + str(catan_game.get_turn() + 1) + \", action:\" + str(action))\n",
    "\n",
    "print(catan_game.game.board)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eda2aad",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Model\n",
    "\n",
    "// _describe in detail how your model of the above domain, including objective (e.g. reward function), dynamics (e.g., transition function), states, observations, actions, etc. (refer to the lecture [Models in AI](https://moodle.technion.ac.il/pluginfile.php/1831187/mod_resource/content/2/236606_ModelsInAI.pdf))_\n",
    "\n",
    "We model the environment as an MDP\n",
    "\n",
    "$\\mathfrak{X}$ - $\\{[current_player, dice, longest_road, initializtion_stage, intersection_buildings, roads, resources, harbors]\\}$\n",
    "The state space details can be found under Observations section\n",
    "\n",
    "${\\mathfrak{A}} = \\{\\mathcal{A}^i\\}_{i=1}^{4}$  where $\\mathcal{A}^i(s)$ = \\{\"possible trades\",\"possible buildings(cities/settlements/roads)\", \"end turn\"\\}\n",
    "$\\newline$ The action space details can be found under Action section. The trading and building possibilities depends on the resources availability. \n",
    "\n",
    "$\\mathfrak{P} (s'|s,a)$ - The probability of getting the state s', when we are in the state s, and make the action a.\n",
    "The stochastic transition happens only when $s.phase = \"dice\" $.\n",
    "The stochasticity is over $s'.resources$ for all players due to a new resources allocation (see the [rules](https://en.wikipedia.org/wiki/Catan) of the game)\n",
    "     \n",
    "$\\mathfrak{R} = \\{\\mathcal{R}^i\\}_{i=1}^{4}$ - reward functions. $\\mathcal{R}^i=\\frac{agent^i\\_ score}{\\sum{scores}}$ \n",
    "    \n",
    "Our objective is to find an optimal policy for our agent that maximizes his utility function.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b57c922",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Solution\n",
    "\n",
    "1. _describe theoretically how you are solving the problem. include here any guarantees (and their proofs) that your solution provides_\n",
    "2. _describe practically your method solving the problem_\n",
    "3. _describe any implementation challenges you faced_\n",
    "\n",
    "\n",
    "<img src=\"images/plan.png\" width=\"900\" />\n",
    "\n",
    "The planning diagram\n",
    "\n",
    "\n",
    "\n",
    "<b>How we solved the problem:</b>\n",
    "Our solution is a combination between a variation of MCTS, and DNN.\n",
    "The DNN role:\n",
    "The roll of the DNN is to estimate the value function (the result of the game).\n",
    "It takes a vectorized representation of a game's state as an input, and returns a prediction of all the players' results at the end of the game, when the game stats from the given state.\n",
    "Each training game, we create a dataset with all the states we visited along the game, and with the result of the game as the same label of each sample.\n",
    "At the end of each game, we train the DNN to improve its predictions in the next game.\n",
    "\n",
    "The MCTS role:\n",
    "Each train along the training game, we use a variation of MCTS to choose the action of the current player.\n",
    "\n",
    "The variation of the MCTS:\n",
    "Contrary to the regular MCTS, our MCTS consists of more than one player, as mentioned in the diagram.\n",
    "Each layer of the MCTS, belongs to other player, and the order of the players determined according to their order in the game.\n",
    "In addition, instead of states nodes only, our MCTS contains actions nodes too.\n",
    "\n",
    "The selection phase:\n",
    "The selection phase in our MCTS, chooses the next node according to the UCT criterion, but from the current player perspective. That is, different players will get different UCT values of the same node.\n",
    "In our MCTS, the UCT value will be calculated by the sum of the rewards of the CURRENT PLAYER.\n",
    "\n",
    "The insertion phase:\n",
    "Contrary to the regular MCTS, our version inserts not only the new state, but also the possible actions from it.\n",
    "\n",
    "The simulation phase:\n",
    "In our version, instead of the simulation phase, we use the DNN to predict the result of the game from the current state.\n",
    "\n",
    "The propagation phase:\n",
    "The propagation phase of our version will look the same as the original one, but include the updates of the action nodes.\n",
    "Each action node, will get the same value of its parent state node.\n",
    "\n",
    "The guarantees of our model:\n",
    "Because we rely on the DNN to predict the result of the game, the results of our algorithm depend on the inclusion ability of the DNN, so there is no guarantees about the executions of our algorithm.\n",
    "\n",
    "\n",
    "<b>How practically our method solves the problem:</b>\n",
    "As we mentioned above, the roll of the DNN is to learn from sets of states and the end results.\n",
    "If the inclusion ability of the DNN will be satisfying, it will improve its predictions every game, and better actions will be taken by the MCTS.\n",
    "\n",
    "\n",
    "<b>Implementation challenges:</b>\n",
    "* In the original version of the UCT, we should give the highest priority to non visited nodes. Catan game has a big branching factor, and it would take a lot of time to explore all the unvisited nodes in the tree. we faced with this issue by using some pruning that will shrink the exploration part of the UCT, to make it possible with our hardware limitations.\n",
    "\n",
    "* At the beginning of the training, the DNN initialized with random weights. so all the players took random actions along the first game, and close to random actions in the next few games. We found out that there is a big chance to run the game forever when all the players play randomly. We faced this issue by adding a heuristic function to the DNN result, to make the trains a little more sophisticated, to make the first games done.\n",
    "\n",
    "* When the selection phase in the MCTS chooses to take some action, we have to know what will be the given state after this action.\n",
    "In Catan, the given state depends on the cubes result after making an action, so we couldn't know what will be the given action.\n",
    "We couldn't choose one of the possible states randomly, because their distribution isn't uniform.\n",
    "We faced with this issue by simulating the chosen action, and taking the given state in the simulator. That way it will converge to the right distribution of the possible states,\n",
    "\n",
    "\n",
    "TODO(Dan): We can look at the git to see what has been changed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "699493ed",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# TODO: show core functions and classes for your solution\n",
    "#   - must be clean, tidy, and well documented code.\n",
    "#   - do not add basic tools and utilities. put those in your code base and import.\n",
    "#   - you may display your code in markdown instead if you don't wish to run it,\n",
    "#     and prefer to import it from your code base\n",
    "\n",
    "# TODO: add MCTS main functions with comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b520503",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# TODO: show a running example of your method\n",
    "#   - run algorithm (e.g. training) or load pre-calculated values (e.g. model weights) \n",
    "#   - visualize run (if applicable)\n",
    "#   - show results for this run\n",
    "#   - TIP: use a constant random seed to ensure deterministic results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ed190c86",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Parameters definition\n",
    "\"\"\"\n",
    "# training parms\n",
    "hp_model_training = dict(loss_fn=torch.nn.MSELoss(),\n",
    "                         batch_size=100,\n",
    "                         num_epochs=100,\n",
    "                         test_ratio=0.2,\n",
    "                         valid_ratio=0.2,\n",
    "                         early_stopping=100)\n",
    "# optimizer params\n",
    "hp_optimizer = dict(lr=0.001,\n",
    "                    weight_decay=0.01,\n",
    "                    momentum=0.99)\n",
    "# NN structure params\n",
    "hp_model = dict(hidden_layers_num=1,\n",
    "                hidden_layers_size=20,\n",
    "                activation='relu')\n",
    "\n",
    "#MCTS params: c - UCT exploration/exploitation param, d-weight on heuristic importance against the model(NN)\n",
    "hp_mcts = dict(c=1,\n",
    "               d=3,\n",
    "               iterations_num=200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "542dda8e",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# NN creation\n",
    "def create_model(in_dim, out_dim, model_file):\n",
    "    if os.path.isfile(model_file):\n",
    "        print(f'loading model from \"{model_file}\"...')\n",
    "        mlp = torch.load(model_file)\n",
    "        print(mlp)\n",
    "        return mlp\n",
    "\n",
    "    mlp = MLP(\n",
    "        in_dim=in_dim,\n",
    "        dims=[hp_model['hidden_layers_size']] * hp_model['hidden_layers_num'] + [out_dim],\n",
    "        nonlins=[hp_model['activation']] * hp_model['hidden_layers_num'] + ['none']\n",
    "    )\n",
    "\n",
    "    print('creating model...')\n",
    "    print(mlp)\n",
    "    return mlp\n",
    "\n",
    "\n",
    "# training function\n",
    "def train(dl_train, dl_valid, dl_test, model):\n",
    "    loss_fn = hp_model_training['loss_fn']\n",
    "    optimizer = torch.optim.SGD(params=model.parameters(), **hp_optimizer)\n",
    "    trainer = MLPTrainer(model, loss_fn, optimizer)\n",
    "\n",
    "    return trainer.fit(dl_train,\n",
    "                       dl_valid,\n",
    "                       num_epochs=hp_model_training['num_epochs'],\n",
    "                       print_every=10,\n",
    "                       early_stopping=hp_model_training['early_stopping'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f567d99c",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Agent holds the model it will be used to make decisions under the MCTS.\n",
    "Each agent can be use different model to predict the value function on a given state.\n",
    "\"\"\"\n",
    "class Agent:\n",
    "    def __init__(self, model, prune=True):\n",
    "        self.model=model\n",
    "        self.prune=prune #boolean parmeter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f553209d",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def train_agent(games_num, model_path):\n",
    "    model = create_model(Catan.get_state_size(), Catan.get_players_num(), model_path)\n",
    "    agents = Catan.get_players_num()*[Agent(model)]\n",
    "\n",
    "    for i in range(1, games_num + 1):\n",
    "        print(f'_________________game {i}/{games_num}________________')\n",
    "\n",
    "        catan_game = Catan()\n",
    "\n",
    "        ds = Dataset(hp_model_training['batch_size'], hp_model_training['valid_ratio'], hp_model_training['test_ratio'])\n",
    "\n",
    "        turns_num = 0\n",
    "        actions_num = 0\n",
    "\n",
    "        while True:\n",
    "            actions_num += 1\n",
    "\n",
    "            best_action = mcts_get_best_action(catan_game, agents, hp_mcts['c'], hp_mcts['d'], hp_mcts['iterations_num'])\n",
    "            print(\"Player \" + str(catan_game.get_turn() + 1) + \", action:\" + str(best_action))\n",
    "\n",
    "            reward = catan_game.make_action(best_action)\n",
    "            if best_action[0] == 4:\n",
    "                turns_num += 1\n",
    "\n",
    "                print(\"Player \" + str(catan_game.get_turn() + 1) + \" turn!, dice: \" + str(catan_game.dice))\n",
    "                # print(catan_game.game.board)\n",
    "\n",
    "            ds.add_sample(catan_game.get_state())\n",
    "            if catan_game.is_over() or actions_num > 600:\n",
    "                if actions_num > 600:\n",
    "                  print(\"No winner, Final board:\")\n",
    "                  print(catan_game.game.board)\n",
    "                  reward = [catan_game.game.get_victory_points(catan_game.game.players[i]) for i in range (Catan.get_players_num())]\n",
    "                else:\n",
    "                  print(\"Congratulations! Player %d wins!\" % (catan_game.cur_id_player + 1))\n",
    "                  print(\"Final board:\")\n",
    "                  print(catan_game.game.board)\n",
    "\n",
    "\n",
    "                ds.set_label(reward)\n",
    "\n",
    "                dl_train, dl_valid, dl_test = ds.get_data_loaders()\n",
    "                fit_res = train(dl_train, dl_valid, dl_test, model)\n",
    "                plot_fit(fit_res, log_loss=False, train_test_overlay=True)\n",
    "                plt.show()\n",
    "                print(ds)\n",
    "\n",
    "                print(f'saving model in \"{model_path}\"')\n",
    "                torch.save(model, model_path)\n",
    "                break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e488eb85",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading model from \"src/model2\"...\n",
      "MLP(\n",
      "  (mlp_layers): Sequential(\n",
      "    (0): Linear(in_features=160, out_features=20, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=20, out_features=4, bias=True)\n",
      "    (3): Identity()\n",
      "  )\n",
      ")\n",
      "_________________game 1/2________________\n"
     ]
    }
   ],
   "source": [
    "train_agent(2, \"src/model2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "723d0b2d",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Evaluation\n",
    "\n",
    "_define the evaluation metrics used to measure the sucess of your experiments. Explain why they are relevant for your problem_\n",
    "\n",
    "1. After playing number of games, we trained our neural network and then run multiple games in order to collects statistics over agents' winning rate.\n",
    "2. The evaluation has been done by comparing different types of agents:<br>\n",
    "     a. $Agent 1$ - Trained NN with pruning.<br>\n",
    "     b. $Agent 2$ - Trained NN without pruning.<br>\n",
    "     c. $Agent 3$ - No NN with pruning.<br>\n",
    "     d. $Agent 4$ - No NN without pruning.<br>\n",
    "     c. $Agent 5$ - Random actions.<br>\n",
    "\n",
    "* Agents 1-4 is based on the MCTS, with 300 iterations, using heuristic.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b31312f",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def test_agent(games_num):\n",
    "    model1 = create_model(Catan.get_state_size(), Catan.get_players_num(), 'model')\n",
    "    model2 = create_model(Catan.get_state_size(), Catan.get_players_num(), 'src/model2')\n",
    "    agents = [Agent(model1,prune=False), Agent(model1,prune=False), Agent(model1), Agent(model2)]\n",
    "    stats = {k: [] for k in range(Catan.get_players_num())}\n",
    "    for i in range(1, games_num + 1):\n",
    "        print(f'_________________game {i}/{games_num}________________')\n",
    "        catan_game = Catan()\n",
    "        actions_num = 0\n",
    "        turns_num = 0\n",
    "        while True:\n",
    "            actions_num += 1\n",
    "            if catan_game.get_turn() == 0:\n",
    "                actions = catan_game.get_actions(prune=False)\n",
    "                best_action = random.choice(actions)\n",
    "                # reward = catan_game.make_action(tuple(best_action))\n",
    "\n",
    "            else:\n",
    "                best_action = mcts_get_best_action(catan_game, agents, hp_mcts['c'], hp_mcts['d'], hp_mcts['iterations_num'])\n",
    "\n",
    "            print(\"Player \" + str(catan_game.get_turn() + 1) + \", action:\" + str(best_action))\n",
    "            reward = catan_game.make_action(best_action)\n",
    "            if best_action[0] == 4:\n",
    "                turns_num += 1\n",
    "                print(\"Player \" + str(catan_game.get_turn() + 1) + \" turn!, dice: \" + str(catan_game.dice))\n",
    "                # print(catan_game.game.board)\n",
    "\n",
    "            if catan_game.is_over() or actions_num > 600:\n",
    "                if actions_num > 600:\n",
    "                  print(\"No winner, Final board:\")\n",
    "                  print(catan_game.game.board)\n",
    "                else:\n",
    "                  stats[catan_game.cur_id_player].append([actions_num, int(turns_num/4)])\n",
    "                  print(\"Congratulations! Player %d wins!\" % (catan_game.cur_id_player + 1))\n",
    "                  print(\"Final board:\")\n",
    "                  print(catan_game.game.board)\n",
    "                break\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d24ef5a",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Results\n",
    "\n",
    "1. _describe what you expected to see in your results._\n",
    "2. _describe your results and cmopare them to your expectations._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "798cf9a5",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "Missing parentheses in call to 'print'. Did you mean print(with agents description)? (383974473.py, line 6)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;36m  Input \u001B[1;32mIn [49]\u001B[1;36m\u001B[0m\n\u001B[1;33m    print with agents description\u001B[0m\n\u001B[1;37m          ^\u001B[0m\n\u001B[1;31mSyntaxError\u001B[0m\u001B[1;31m:\u001B[0m Missing parentheses in call to 'print'. Did you mean print(with agents description)?\n"
     ]
    }
   ],
   "source": [
    "num_of_tests = 4\n",
    "winning_records = test_agent(num_of_tests)\n",
    "stats= [(100*len(v)/num_of_tests) for k,v in winning_records.items()]\n",
    "print(\"winning rate(prectnage)\")\n",
    "print(stats)\n",
    "print with agents description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a2af6cd1",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "winning rate(prectnage)\n",
      "[0.0, 0.0, 100.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "winning_records = {0: [], 1: [], 2: [[156, 21], [216, 26]], 3: []}\n",
    "stats= [(100*len(v)/2) for k,v in winning_records.items()]\n",
    "print(\"winning rate(prectnage)\")\n",
    "print(stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f527e0ec",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# TODO: visualize reults\n",
    "#   - tables of evaluation method\n",
    "#   - compare to previous works (if results available)\n",
    "#   - any other graphs / charts / plot that visualize your method's performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c5fb832",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Method Limitations / Possible Future Extensions\n",
    "\n",
    "_describe where any limitations or drawbacks to your method, as well as any suggestions for future work that will improve upon it._\n",
    "\n",
    "we need a DQN for the initial placement stage.\n",
    "No utilization of the full game, the settings are pretty shallow\n",
    "no exact strategy\n",
    "didn't test it against humam player or other methods such as Jsettlers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340d282e-f82e-4f42-9a68-9643774e35e3",
   "metadata": {
    "id": "340d282e-f82e-4f42-9a68-9643774e35e3"
   },
   "outputs": [],
   "source": [
    "# TODO: visualize reults\n",
    "#   - tables of evaluation method\n",
    "#   - compare to previous works (if results available)\n",
    "#   - any other graphs / charts / plot that visualize your method's performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c634502a-1d68-4e7d-85ff-985a64f04ef9",
   "metadata": {
    "id": "c634502a-1d68-4e7d-85ff-985a64f04ef9"
   },
   "source": [
    "## Method Limitations / Possible Future Extensions\n",
    "\n",
    "_describe where any limitations or drawbacks to your method, as well as any suggestions for future work that will improve upon it._\n",
    "\n",
    "we need a DQN for the initial placement stage.\n",
    "No utilization of the full game, the settings are pretty shallow\n",
    "no exact strategy\n",
    "didn't test it against humam player or other methods such as Jsettlers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26de0341-784f-4384-b8df-eeeee64e461f",
   "metadata": {
    "id": "26de0341-784f-4384-b8df-eeeee64e461f"
   },
   "outputs": [],
   "source": [
    "# TODO: show a running example that illustrates the above limitations"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "CatanNB.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}