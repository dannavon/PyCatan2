{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bcb93415-955d-4398-9636-93f4ceaa05bb",
   "metadata": {
    "id": "bcb93415-955d-4398-9636-93f4ceaa05bb"
   },
   "source": [
    "# Settelers of catan inspired by Alphazero approach\n",
    "Aviv Cohen; avivcohen@campus.technion.ac.il\n",
    "\n",
    "Dan Navon; danavon@campus.technion.ac.il"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4084f25-7893-45d9-9522-9b134f38878e",
   "metadata": {
    "id": "a4084f25-7893-45d9-9522-9b134f38878e"
   },
   "source": [
    "## Introduction\n",
    "\n",
    "An introduction should include the following:\n",
    "- An informal description of the problem you are trying to solve (examples are best)\n",
    "- Motivation for solving this problem\n",
    "- An explanation why it is hard to solve this problem\n",
    "- Previous methods for solving the problem and their strengths and weaknesses\n",
    "- A description of your solution to the problem, how it overcomes the issues in previous mehtods, and what new issues arise.\n",
    "- A short description on how you intend to evaluate your solution."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### An informal description of the problem you are trying to solve (examples are best\n",
    "Catan is a resource driven game in which players roll dice to collect resources, trade materials with other players, and\n",
    "compete to be the first one to achieve 10 points.\n",
    "Our goal is to create an agent that improves his strategy to this game, while playing against himself.\n",
    "Quick explanation of Catan can be found in [YouTube: How to Play Catan in 4 Minutes - Rules Girl](https://www.youtube.com/watch?v=4fUa_ZJ7beM)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "# Youtube\n",
    "HTML('<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/watch?v=4fUa_ZJ7beM\" frameborder=\"0\" allowfullscreen></iframe>')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Motivation for solving this problem and why is it hard\n",
    "While dealing well with traditional games, these techniques are often unsatisfactory for modern strategic games,\n",
    "commonly called Eurogames, because of the greater complexity of these games when compared to traditional board games [1].\n",
    "Eurogame archetype, with gameplay elements that make it challenging for traditional tree search algorithms, such as\n",
    "Minimax: imperfect information, randomly determined moves, more than 2 players and negotiation between players. Most\n",
    "autonomous agent players available for this game have game-specific heuristics and have a low win-rate against human players.\n",
    "\n",
    "\n",
    "[1] - D. Robilliard and C. Fonlupt, Towards Human-Competitive\n",
    "Game Playing for Complex Board Games with Genetic\n",
    "Programming. Cham: Springer International Publishing,\n",
    "2016, pp. 123–135."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Previous methods for solving the problem and their strengths and weaknesses:\n",
    "\n",
    "[JSettlers](https://github.com/jsettlers/settlers-remake) is an open-source Java implementation of Settlers of Catan that [includes implementations of AI agents](https://www.semanticscholar.org/paper/Real-time-decision-making-for-adversarial-using-a-Hammond-Thomas/bee030f91fe1074548e58fbebc92d2b10c90bc1d) that are frequently used as benchmarks for new game playing strategies.\n",
    "\n",
    "[QSettlers,by Peter McAughan in 2019](https://akrishna77.github.io/QSettlers/) in this work, the authors attempted to apply the DQN paradigm to develop an AI model to play and win Settlers of Catan.\n",
    "Although the team was able to train and develop a working DQN model specifically for the player trading mechanism of the game they couldn't implement a working DQN model for general.<br>\n",
    "<img src=\"docs/images/DQN_trades.jpg\" width=\"600\" align=\"center\"/>\n",
    "\n",
    "At [Re-L Catan: Evaluation of Deep Reinforcement Learning for Resource Management Under Competitive and Uncertain Environments](https://cs230.stanford.edu/projects_fall_2021/reports/103176936.pdf) the authors tried to take the Qsettlers method into a general gamplay by creating different DQN for each part of the game.<br>\n",
    "The paper lack the explanation on how they connected these NNs, but it seems to perform solidly on a game server named www.colonist.io.\n",
    "<img src=\"docs/images/RE-L.jpg\" width=\"600\" align=\"center\"/>\n",
    "\n",
    "These last 2 paper led us to abandon the DQN approach due to the incomplete view over the game.\n",
    "\n",
    "[Optimizing UCT for Settlers of Catan](https://www.sbgames.org/sbgames2017/papers/ComputacaoFull/175405.pdf) extends the\n",
    " rules' simplification assumed at [Monte-Carlo Tree Search in Settlers of Catan](https://www.researchgate.net/publication/220716999_Monte-Carlo_Tree_Search_in_Settlers_of_Catan),\n",
    "  The former paper uses a combination of pruning strategy that uses domain knowledge to reduce the algorithm’s search\n",
    "   space and trade-optimistic search heuristic.\n",
    "<img src=\"docs/images/MovePruning.jpg\" width=\"600\" align=\"center\"/>\n",
    "\n",
    "The problem with this approach is that it heavily computation demanding due to the length of each rollout until the end of game times the number of iteration times the length of the actual game.<br>\n",
    "Secondly it doesn't have any learning process between games as in Alphazero.\n",
    "\n",
    "[Mastering the game of Go with deep neural networks and tree search](https://www.nature.com/articles/nature16961.pdf) is a well known algorithm which relay on MCTS but replacing the rollouts with a CNN in order to tackle these 2 last weaknesses. <br>\n",
    "The Alphazero algorithm assumes full observability and only 2 agents unlike the Catan game which may be played as a 4 players game. <br>\n",
    "\n",
    "It is worth to mention [Game strategies for The Settlers of Catan](https://ieeexplore.ieee.org/document/6932884) which gives a surevy over different game strategies, this paper gave us another point of view although we didn't actually use it.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### A description of your solution to the problem, how it overcomes the issues in previous mehtods, and what new issues arise.\n",
    "Ultimately, our approach is a combination of simplified Alphazero and the hueristic+pruning method shown at \"Optimizing UCT for Settlers of Catan\".\n",
    "We want to tak advantage of the great success of Alphazero in other games as \"GO\" with the abillity to save execution\n",
    "time with the DNN prediction instead of rollouts and we want to use the approach of the MCTS with heuristics+pruning in order to shrink the search space.\n",
    "\n",
    "2 problems arise with this approach:\n",
    "1. We need to change some game's rules in order to be compatible with the Alphazero algorithm\n",
    "2. We don't have the computational power \"DeepMind\" have and therefore we won't be able to produce good results as\n",
    "AlphaGo, some projects as [Leela Zero](https://github.com/leela-zero/leela-zero) has tried to duplicate AlphaGO results\n",
    "without success due to the amount of computational resources required.\n",
    "\n",
    "We see this project as way to learning and interact with these new algorithms and not to achieve the most successful algorithm.\n",
    "\n",
    "\n",
    "### A short description on how you intend to evaluate your solution.\n",
    "Since we slightly chaged the rules of the game we cannot compare our algorithm against other AI agents as [JSettlers](https://github.com/jsettlers/settlers-remake)\n",
    " or [colonist.io](www.colonist.io) and other games servers with bots.\n",
    "\n",
    "Therefore, we will try to examine our assumption as - using trained NN should perform better than no NN( and no rollouts) while using the MCTS variant \"AlphaZero\" proposed.\n",
    " Or the less trivial - using pruning and/or heuristic or not. the reason it is not trivial is that we might trim actions that can give a better result.\n",
    " We will test it by playing different types of agents between themselves."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Motivation for solving this problem and why is it hard"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Previous methods for solving the problem and their strengths and weaknesses:\n",
    "\n",
    "[JSettlers](https://github.com/jsettlers/settlers-remake) is an open-source Java implementation of Settlers of Catan that [includes implementations of AI agents](https://www.semanticscholar.org/paper/Real-time-decision-making-for-adversarial-using-a-Hammond-Thomas/bee030f91fe1074548e58fbebc92d2b10c90bc1d) that are frequently used as benchmarks for new game playing strategies.\n",
    "\n",
    "[QSettlers,by Peter McAughan in 2019](https://akrishna77.github.io/QSettlers/) in this work, the authors attempted to apply the DQN paradigm to develop an AI model to play and win Settlers of Catan.\n",
    "Although the team was able to train and develop a working DQN model specifically for the player trading mechanism of the game they couldn't implement a working DQN model for general.<br>\n",
    "<img src=\"docs/images/DQN_trades.jpg\" width=\"600\" align=\"center\"/>\n",
    "\n",
    "At [Re-L Catan: Evaluation of Deep Reinforcement Learning for Resource Management Under Competitive and Uncertain Environments](https://cs230.stanford.edu/projects_fall_2021/reports/103176936.pdf) the authors tried to take the Qsettlers method into a general gamplay by creating different DQN for each part of the game.<br>\n",
    "The paper lack the explanation on how they connected these NNs, but it seems to perform solidly on a game server named www.colonist.io.\n",
    "<img src=\"docs/images/RE-L.jpg\" width=\"600\" align=\"center\"/>\n",
    "\n",
    "These last 2 paper led us to abandon the DQN approach due to the incomplete view over the game.\n",
    "\n",
    "[Optimizing UCT for Settlers of Catan](https://www.sbgames.org/sbgames2017/papers/ComputacaoFull/175405.pdf) extends the\n",
    " rules' simplification assumed at [Monte-Carlo Tree Search in Settlers of Catan](https://www.researchgate.net/publication/220716999_Monte-Carlo_Tree_Search_in_Settlers_of_Catan),\n",
    "  The former paper uses a combination of pruning strategy that uses domain knowledge to reduce the algorithm’s search\n",
    "   space and trade-optimistic search heuristic.\n",
    "<img src=\"docs/images/MovePruning.jpg\" width=\"600\" height=200 align=\"center\"/>\n",
    "\n",
    "The problem with this approach is that it heavily computation demanding due to the length of each rollout until the end of game times the number of iteration times the length of the actual game.<br>\n",
    "Secondly it doesn't have any learning process between games as in Alphazero.\n",
    "\n",
    "[Mastering the game of Go with deep neural networks and tree search](https://www.nature.com/articles/nature16961.pdf) is a well known algorithm which relay on MCTS but replacing the rollouts with a CNN in order to tackle these 2 last weaknesses. <br>\n",
    "The Alphazero algorithm assumes full observability and only 2 agents unlike the Catan game which may be played as a 4 players game. <br>\n",
    "\n",
    "It is worth to mention [Game strategies for The Settlers of Catan](https://ieeexplore.ieee.org/document/6932884) which gives a surevy over different game strategies, this paper gave us another point of view although we didn't actually use it.\n",
    "\n",
    "### A description of your solution to the problem, how it overcomes the issues in previous mehtods, and what new issues arise.\n",
    "Ultimately, our approach is a combination of simplified Alphazero and the hueristic+prunning method shown at \"Optimizing UCT for Settlers of Catan\".\n",
    "We want to tak advantage of the great success of Alphazero in other games as \"GO\" with the abillity to save execution time with the DNN prediction instead of rollouts and we want to use the approach of the MCTS with heuristics+prunning in order to shrink the search space.\n",
    "\n",
    "2 problems arise with this approach:\n",
    "1. We need to change some of the game rules in order to be compatible with the Alphazero algorithm\n",
    "2. We don't have the computational power \"DeepMind\" have and therefore we won't be able to produce good results as AlphaGo, some projects tried to\n",
    "\n",
    "\n",
    "### A short description on how you intend to evaluate your solution.\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "id": "ce84e3a4",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "### Previous methods for solving the problem and their strengths and weaknesses:\n",
    "\n",
    "[JSettlers](https://github.com/jsettlers/settlers-remake) is an open-source Java implementation of Settlers of Catan that [includes implementations of AI agents](https://www.semanticscholar.org/paper/Real-time-decision-making-for-adversarial-using-a-Hammond-Thomas/bee030f91fe1074548e58fbebc92d2b10c90bc1d) that are frequently used as benchmarks for new game playing strategies.\n",
    "\n",
    "[QSettlers,by Peter McAughan in 2019](https://akrishna77.github.io/QSettlers/) in this work, the authors attempted to apply the DQN paradigm to develop an AI model to play and win Settlers of Catan.\n",
    "Although the team was able to train and develop a working DQN model specifically for the player trading mechanism of the game they couldn't implement a working DQN model for general.<br>\n",
    "<img src=\"docs/images/DQN_trades.jpg\" width=\"600\" align=\"center\"/>\n",
    "\n",
    "At [Re-L Catan: Evaluation of Deep Reinforcement Learning for Resource Management Under Competitive and Uncertain Environments](https://cs230.stanford.edu/projects_fall_2021/reports/103176936.pdf) the authors tried to take the Qsettlers method into a general gamplay by creating different DQN for each part of the game.<br>\n",
    "The paper lack the explanation on how they connected these NNs, but it seems to perform solidly on a game server named www.colonist.io.\n",
    "<img src=\"docs/images/RE-L.jpg\" width=\"600\" align=\"center\"/>\n",
    "\n",
    "These last 2 paper led us to abandon the DQN approach due to the incomplete view over the game.\n",
    "\n",
    "[Optimizing UCT for Settlers of Catan](https://www.sbgames.org/sbgames2017/papers/ComputacaoFull/175405.pdf) extends the\n",
    " rules' simplification assumed at [Monte-Carlo Tree Search in Settlers of Catan](https://www.researchgate.net/publication/220716999_Monte-Carlo_Tree_Search_in_Settlers_of_Catan),\n",
    "  The former paper uses a combination of pruning strategy that uses domain knowledge to reduce the algorithm’s search\n",
    "   space and trade-optimistic search heuristic.\n",
    "<img src=\"docs/images/MovePruning.jpg\" width=\"600\" height=200 align=\"center\"/>\n",
    "\n",
    "The problem with this approach is that it heavily computation demanding due to the length of each rollout until the end of game times the number of iteration times the length of the actual game.<br>\n",
    "Secondly it doesn't have any learning process between games as in Alphazero.\n",
    "\n",
    "[Mastering the game of Go with deep neural networks and tree search](https://www.nature.com/articles/nature16961.pdf) is a well known algorithm which relay on MCTS but replacing the rollouts with a CNN in order to tackle these 2 last weaknesses. <br>\n",
    "The Alphazero algorithm assumes full observability and only 2 agents unlike the Catan game which may be played as a 4 players game. <br>\n",
    "\n",
    "It is worth to mention [Game strategies for The Settlers of Catan](https://ieeexplore.ieee.org/document/6932884) which gives a surevy over different game strategies, this paper gave us another point of view although we didn't actually use it.\n",
    "\n",
    "### A description of your solution to the problem, how it overcomes the issues in previous mehtods, and what new issues arise.\n",
    "Ultimately, our approach is a combination of simplified Alphazero and the hueristic+prunning method shown at \"Optimizing UCT for Settlers of Catan\".\n",
    "We want to tak advantage of the great success of Alphazero in other games as \"GO\" with the abillity to save execution time with the DNN prediction instead of rollouts and we want to use the approach of the MCTS with heuristics+prunning in order to shrink the search space.\n",
    "\n",
    "2 problems arise with this approach:\n",
    "1. We need to change some of the game rules in order to be compatible with the Alphazero algorithm\n",
    "2. We don't have the computational power \"DeepMind\" have and therefore we won't be able to produce good results as AlphaGo, some projects tried to\n",
    "\n",
    "\n",
    "### A short description on how you intend to evaluate your solution.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "756bb780",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# TODO: provide a step-by-step guide to running your domain in code\n",
    "#  - initialize an environment instance\n",
    "#  - visualize observations ()\n",
    "#  - show how to take an action.\n",
    "#  - play a game / an episode / several timesteps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a76bdee",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Domain - Catan\n",
    "\n",
    "We forked our project from [PyCatan2](https://github.com/josefwaller/PyCatan2), which gives a raw implmentation of the game in order to let other developers to implment the rules they inted to use.\n",
    "In our case, we implmented the very basic game elements - building cities, settelments, roads and trading (no players' trading).\n",
    "The reason is to keep the game state fully visible as in other AlphaZero games implementation (Go, Chess, 4-in a row, tic-tac-toe and etc) so we can be compatible with the original game.\n",
    "\n",
    "Full explanation of the game can be found [here](https://en.wikipedia.org/wiki/Catan) or under docs directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "32e8cb61",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os.path\n",
    "import random\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from src.mcts import mcts_get_best_action\n",
    "from src.mlp import MLP\n",
    "from src.dataset import Dataset\n",
    "from src.training import MLPTrainer\n",
    "from src.plot import plot_fit\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "580439f7",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We wrapped the PyCatan2 in order to let the agents interact with game.\n",
    "\n",
    "One can change <code>catan_wrp.py</code> , add or remove different rules of the game, change the initial board and etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "625d2ee2",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Board:\n",
      "                                                       \n",
      "                                                       \n",
      "                                                       \n",
      "                                                       \n",
      "                 3:1         2:1                       \n",
      "                  .--'--.--'--.--'--.                  \n",
      "                  | 10  |  2  |  9  | 2:1              \n",
      "               .--'--.--'--.--'--.--'--.               \n",
      "           2:1 | 12  |  6  |  4  | 10  |               \n",
      "            .--'--.--'--.--'--.--'--.--'--.            \n",
      "            |  9  | 11  |   R |  3  |  8  | 3:1        \n",
      "            '--.--'--.--'--.--'--.--'--.--'            \n",
      "           2:1 |  8  |  3  |  4  |  5  |               \n",
      "               '--.--'--.--'--.--'--.--'               \n",
      "                  |  5  |  6  | 11  | 2:1              \n",
      "                  '--.--'--.--'--.--'                  \n",
      "                 3:1         3:1                       \n",
      "                                                       \n",
      "                                                       \n",
      "                                                       \n"
     ]
    }
   ],
   "source": [
    "from src.catan_wrp import Catan\n",
    "num_players = 4\n",
    "catan_game = Catan()\n",
    "print(\"Board:\")\n",
    "print(catan_game.game.board)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf48697",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The coordination system called is a skwed 2D grid, more details can be found under <code>/docs/Working-with-Board.srt</code>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c36206b",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<img src=\"docs/images/catangrid_withpieces.png\" align=\"left\"/>\n",
    "<img src=\"docs/images/BeginnerBoard.jpg\" align=\"right\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f969a0",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Observations\n",
    "The observations returned as a seralized vector.\n",
    "The reason for this serialization is that we would like to predict how good a state is using a DNN which have to recive a constanst size input vector.\n",
    "\n",
    "observations represent = <code>[current_player, dice, longest_road, initializtion_stage, intersection_buildings, roads, resources, harbors] </code>.\n",
    "1. <code>current_player</code> - current player id.\n",
    "2. <code>dice</code> - dice value can be ranged between 2-12.\n",
    "3. <code>longest_road</code> - the player id who owns the longest road (2 victory points).\n",
    "4. <code>initializtion_stage</code> - boolean value, indicates if we are at the initialization stage.\n",
    "5. <code>intersection_buildings</code> - vector with 55 elements where values represents the 55 intersections along the board. values repesented as $player\\_id*10 + building\\_type$ where $building\\_type$ may be- 1-Settlement, 2-City. if no bulding han the value is - 0 - No building.\n",
    "for example the value \"12\" means that the intersection belongs to player_id==int(12/10)==1 with a city(12%10==2) on it.\n",
    "6. <code>roads</code> - vector with 71 elements - represnts all posiible locations to place roads at. values may be - 0 - No road, otherwise - player_id+1 where the player_id represnt who onws this road.\n",
    "7. <code>resources</code> - there are 5 different types of resources (lumber, brick, ore, grain, grass), assuming 4 players in the game, this vector will have 20 elments where each elments represnts the amount of each resource\n",
    "8. <code>harbors</code> - 9 elements, the values are as in intersection_buildings, harbors change the trading rates from 4/1 to 3/1\n",
    " \n",
    "overall the state is reprensted in a compcat 160 essential elements.\n",
    "\n",
    "at this image, green dots represnting the intersection, between each 2 adjacent dots, a road can be placed\n",
    "\n",
    "<img src=\"docs/images/intersections.png\"/>\n",
    "\n",
    "Image taken from [\"RE-L Catan\"](https://cs230.stanford.edu/projects_fall_2021/reports/103176936.pdf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1e30527c",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state size:160\n",
      "tensor([ 2.,  7.,  0.,  0.,  0.,  0.,  0.,  0., 21.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., 11.,  0.,  0., 11.,\n",
      "         0.,  1.,  0.,  0., 21.,  0.,  0.,  0., 31.,  0.,  0.,  0.,  0.,  0.,\n",
      "        31.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  3.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         3.,  3.,  0.,  1.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  4.,  4.,  0.,  0.,  0.,  3.,  0.,  2.,  2.,  0.,  0.,  4.,  0.,\n",
      "         0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  1.,  0.,  1.,  2.,  0.,\n",
      "         0.,  0.,  0.,  1.,  3.,  2.,  0.,  0.,  0.,  2.,  1.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.])\n"
     ]
    }
   ],
   "source": [
    "state = catan_game.get_state()\n",
    "print(\"state size:\" + str(len(state)))\n",
    "print(state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73978b18",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Actions\n",
    "The actions space contains - building(city / settelement/ road), trading or end turn(+dice roll)\n",
    "Not all the action always avilable, the building and trading avilabilty depends on the number of resources each player has\n",
    "\n",
    "The first 8 turns (assuming 4 players) are being played in a different way.\n",
    "The order of the players is 1-2-3-4-4-3-2-1.\n",
    "This stage is the picking stage where each player pick a settlement and a road. at the reverse order (4-3-2-1) the players recive resources as well according to the surround hexes. \n",
    "Becuase each player can pick any leagal settlement at the beggining, the number of actions can be up to 55.\n",
    "\n",
    "Each action is represented as a tuple where the first element reflects the type of action and the second the coordinaton of the action.\n",
    "\n",
    "0. build a road\n",
    "1. build a settlement\n",
    "2. build a city\n",
    "3. trade resources (second value is the type of the trade)\n",
    "4. end turn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5758d712",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, ((<Resource.WOOL: 2>, -4), (<Resource.LUMBER: 0>, 1)))\n",
      "(3, ((<Resource.WOOL: 2>, -4), (<Resource.GRAIN: 3>, 1)))\n",
      "(3, ((<Resource.WOOL: 2>, -4), (<Resource.BRICK: 1>, 1)))\n",
      "(3, ((<Resource.WOOL: 2>, -4), (<Resource.ORE: 4>, 1)))\n",
      "(4,)\n",
      "                                                       \n",
      "                                                       \n",
      "                                                       \n",
      "                                                       \n",
      "                 3:1         2:1                       \n",
      "                  .--'--.--'--.--'--s                  \n",
      "                  | 10  |  2  |  9  | 2:1              \n",
      "               s--'--s--'--.--'--.--'--s               \n",
      "           2:1 | 12  |  6  |  4  | 10  |               \n",
      "            .--'--.--'--.--'--c--'--.--'--.            \n",
      "            |  9  | 11  |   R |  3  |  8  | 3:1        \n",
      "            '--.--'--.--'--s--'--.--'--.--s            \n",
      "           2:1 |  8  |  3  |  4  |  5  |               \n",
      "               '--.--'--.--'--.--s--.--'               \n",
      "                  |  5  |  6  | 11  | 2:1              \n",
      "                  '--.--'--.--'--.--'                  \n",
      "                 3:1         3:1                       \n",
      "                                                       \n",
      "                                                       \n",
      "                                                       \n"
     ]
    }
   ],
   "source": [
    "actions = catan_game.get_actions(prune=False)\n",
    "for a in actions:\n",
    "    print(a)\n",
    "best_action = random.choice(actions)\n",
    "catan_game.make_action(best_action)\n",
    "print(catan_game.game.board)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ec3f85",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Game simulation\n",
    "Here are a few moves to adress how the game is progressing.\n",
    "One can use the <code>src/text_game.py</code> in order to play by himself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f0372ff6",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Player 2, action:(<BuildingType.ROAD: 0>, frozenset({(q: -3, r:5), (q: -2, r:5)}))\n",
      "Player 2, action:(<BuildingType.SETTLEMENT: 1>, (q: -3, r:2))\n",
      "Player 3, action:(<BuildingType.ROAD: 0>, frozenset({(q: -3, r:2), (q: -4, r:3)}))\n",
      "Player 3, action:(<BuildingType.SETTLEMENT: 1>, (q: 0, r:1))\n",
      "Player 4, action:(<BuildingType.ROAD: 0>, frozenset({(q: 1, r:0), (q: 0, r:1)}))\n",
      "Player 4, action:(<BuildingType.SETTLEMENT: 1>, (q: 0, r:4))\n",
      "Player 4, action:(<BuildingType.ROAD: 0>, frozenset({(q: 0, r:4), (q: -1, r:4)}))\n",
      "Player 4, action:(<BuildingType.SETTLEMENT: 1>, (q: -1, r:0))\n",
      "Player 3, action:(<BuildingType.ROAD: 0>, frozenset({(q: -1, r:0), (q: 0, r:-1)}))\n",
      "Player 3, action:(<BuildingType.SETTLEMENT: 1>, (q: 3, r:-2))\n",
      "Player 2, action:(<BuildingType.ROAD: 0>, frozenset({(q: 3, r:-2), (q: 4, r:-3)}))\n",
      "Player 2, action:(<BuildingType.SETTLEMENT: 1>, (q: 4, r:-4))\n",
      "Player 1, action:(<BuildingType.ROAD: 0>, frozenset({(q: 3, r:-4), (q: 4, r:-4)}))\n",
      "Player 1, action:(<BuildingType.SETTLEMENT: 1>, (q: 2, r:3))\n",
      "Player 1, action:(<BuildingType.ROAD: 0>, frozenset({(q: 2, r:3), (q: 1, r:3)}))\n",
      "Player 2, action:(4,)\n",
      "Player 3, action:(4,)\n",
      "Player 4, action:(4,)\n",
      "Player 1, action:(4,)\n",
      "Player 2, action:(4,)\n",
      "Player 3, action:(4,)\n",
      "Player 3, action:(<BuildingType.ROAD: 0>, frozenset({(q: 0, r:1), (q: -1, r:1)}))\n",
      "Player 4, action:(4,)\n",
      "Player 4, action:(3, ((<Resource.ORE: 4>, -2), (<Resource.LUMBER: 0>, 1)))\n",
      "Player 1, action:(4,)\n",
      "Player 1, action:(3, ((<Resource.LUMBER: 0>, -3), (<Resource.BRICK: 1>, 1)))\n",
      "Player 2, action:(4,)\n",
      "Player 3, action:(4,)\n",
      "Player 4, action:(4,)\n",
      "Player 1, action:(4,)\n",
      "Player 1, action:(<BuildingType.ROAD: 0>, frozenset({(q: 1, r:3), (q: 0, r:4)}))\n",
      "Player 2, action:(4,)\n",
      "Player 3, action:(4,)\n",
      "Player 3, action:(<BuildingType.ROAD: 0>, frozenset({(q: 1, r:0), (q: 2, r:0)}))\n",
      "Player 4, action:(4,)\n",
      "Player 4, action:(<BuildingType.ROAD: 0>, frozenset({(q: -1, r:3), (q: -1, r:4)}))\n",
      "Player 1, action:(4,)\n",
      "Player 2, action:(4,)\n",
      "Player 2, action:(3, ((<Resource.GRAIN: 3>, -4), (<Resource.BRICK: 1>, 1)))\n",
      "Player 3, action:(4,)\n",
      "                                                       \n",
      "                                                       \n",
      "                                                       \n",
      "                                                       \n",
      "                 3:1         2:1                       \n",
      "                  .--'--.--'--.--'--s                  \n",
      "                  | 10  |  2  |  9  | 2:1              \n",
      "               s--'--s--'--.--'--.--'--s               \n",
      "           2:1 | 12  |  6  |  4  | 10  |               \n",
      "            .--'--.--'--.--'--s--'--.--'--.            \n",
      "            |  9  | 11  |   R |  3  |  8  | 3:1        \n",
      "            '--.--'--.--'--s--'--.--'--.--s            \n",
      "           2:1 |  8  |  3  |  4  |  5  |               \n",
      "               '--.--'--.--'--.--s--.--'               \n",
      "                  |  5  |  6  | 11  | 2:1              \n",
      "                  '--.--'--.--'--.--'                  \n",
      "                 3:1         3:1                       \n",
      "                                                       \n",
      "                                                       \n",
      "                                                       \n"
     ]
    }
   ],
   "source": [
    "for i in range(40):\n",
    "    actions = catan_game.get_actions(prune=False)\n",
    "    action = random.choice(actions)\n",
    "    catan_game.make_action(action)\n",
    "    print(\"Player \" + str(catan_game.get_turn() + 1) + \", action:\" + str(action))\n",
    "\n",
    "print(catan_game.game.board)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc264857",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Model\n",
    "\n",
    "// _describe in detail how your model of the above domain, including objective (e.g. reward function), dynamics (e.g., transition function), states, observations, actions, etc. (refer to the lecture [Models in AI](https://moodle.technion.ac.il/pluginfile.php/1831187/mod_resource/content/2/236606_ModelsInAI.pdf))_\n",
    "\n",
    "We model the environment as an MDP\n",
    "\n",
    "$\\mathfrak{X}$ - $\\{[current_player, dice, longest_road, initializtion_stage, intersection_buildings, roads, resources, harbors]\\}$\n",
    "The state space details can be found under Observations section\n",
    "\n",
    "$\\mathfrak{A} = \\{\\mathcal{A}^i\\}_{i=1}^{4}$  where $\\mathcal{A}^i(s)$ = \\{\"possible trades\",\"possible buildings(cities/settlements/roads)\", \"end turn\"\\}\n",
    "$\\newline$ The action space details can be found under Action section. The trading and building possibilities depends on the resources availability. \n",
    "\n",
    "$\\mathfrak{P} (s'|s,a)$ - The probability of getting the state s', when we are in the state s, and make the action a.\n",
    "The stochastic transition happens only when $s.phase = \"dice\" $.\n",
    "The stochasticity is over $s'.resources$ for all players due to a new resources allocation (see the [rules](https://en.wikipedia.org/wiki/Catan) of the game)\n",
    "     \n",
    "$\\mathfrak{R} = \\{\\mathcal{R}^i\\}_{i=1}^{4}$ - reward functions. $\\mathcal{R}^i=\\frac{agent^i\\_ score}{\\sum{scores}}$ \n",
    "    \n",
    "Our objective is to find an optimal policy for our agent that maximizes his utility function.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b04b2356",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Solution\n",
    "\n",
    "<img src=\"images/plan.png\" width=\"900\" />\n",
    "\n",
    "The planning diagram\n",
    "\n",
    "\n",
    "\n",
    "### How we solved the problem:\n",
    "Our solution is a combination between a variation of MCTS, and DNN.\n",
    "The DNN role:\n",
    "The role of the DNN is to estimate the value function (the result at the end of the game).\n",
    "It takes a vectorized representation of a game's state(described under the observation section) as an input, and returns a prediction of all the players' results at the end of the game, when the game stats from the given state.\n",
    "Each training game, we create a dataset with all the states we visited along the game, and with the result of the game as the same label of each sample.\n",
    "At the end of each game, we train the DNN to improve its predictions in the next game.\n",
    "\n",
    "#### The MCTS role:\n",
    "Each train along the training game, we use a variation of MCTS to choose the action of the current player.\n",
    "\n",
    "#### The variation of the MCTS:\n",
    "Contrary to the regular MCTS, our MCTS consists of more than one player, as mentioned in the diagram.\n",
    "Each layer of the MCTS, belongs to other player, and the order of the players determined according to their order in the game.\n",
    "In addition, instead of states nodes only, our MCTS contains actions nodes too.\n",
    "\n",
    "#### The selection phase:\n",
    "The selection phase in our MCTS, chooses the next node according to the UCT criterion, but from the current player\n",
    " perspective. That is, different players will get different UCT values of the same node.\n",
    "In our MCTS, the UCT value will be calculated by the sum of the rewards of the CURRENT PLAYER.\n",
    "\n",
    "#### The insertion phase:\n",
    "Contrary to the regular MCTS, our version inserts not only the new state, but also the possible actions from it.\n",
    "\n",
    "#### The simulation phase:\n",
    "In our version, instead of the simulation phase, we use the DNN to predict the result of the game from the current state.\n",
    "\n",
    "#### The propagation phase:\n",
    "The propagation phase of our version will look the same as the original one, but include the updates of the action nodes.\n",
    "Each action node, will get the same value of its parent state node.\n",
    "\n",
    "### The guarantees of our model:\n",
    "Because we rely on the DNN to predict the result of the game, the results of our algorithm depend on the inclusion\n",
    " ability of the DNN, so there is no guarantees about the executions of our algorithm.\n",
    "\n",
    "\n",
    "### How practically our method solves the problem:\n",
    "As we mentioned above, the role of the DNN is to learn from sets of states and the end results.\n",
    "If the inclusion ability of the DNN will be satisfying, it will improve its predictions every game, and better actions will be taken by the MCTS.\n",
    "\n",
    "\n",
    "### Implementation challenges:\n",
    "* In the original version of the UCT, we should give the highest priority to non visited nodes. Catan game has a big branching factor, and it would take a lot of time to explore all the unvisited nodes in the tree. we faced with this issue by using pruning (as in \"optimizing UCT for Catan\")  that will shrink the exploration part of the UCT, to make it possible with our hardware limitations.\n",
    "\n",
    "* At the beginning of the training, the DNN initialized with random weights. so all the players took random actions\n",
    "along the first game, and close to random actions in the next few games. We found out that there is a big chance to\n",
    "run the game forever when all the players play randomly. We faced this issue by adding a heuristic function to the DNN\n",
    "result, to make the trains a little more sophisticated, to make the first games done.\n",
    "\n",
    "* When the selection phase in the MCTS chooses to take some action, we have to know what will be the given state after this action.\n",
    "In Catan, the given state depends on the cubes result after making an action, so we couldn't know what will be the given action.\n",
    "We couldn't choose one of the possible states randomly, because their distribution isn't uniform.\n",
    "We faced with this issue by simulating the chosen action, and taking the given state in the simulator. That way it will converge to the right distribution of the possible states,\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f1018658",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# TODO: show core functions and classes for your solution\n",
    "#   - must be clean, tidy, and well documented code.\n",
    "#   - do not add basic tools and utilities. put those in your code base and import.\n",
    "#   - you may display your code in markdown instead if you don't wish to run it,\n",
    "#     and prefer to import it from your code base\n",
    "\n",
    "# TODO: add MCTS main functions with comments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb4ce8b",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "def iteration(root, game, agent, c, d):\n",
    "    \"\"\"\n",
    "    make one iteration of the MCTS\n",
    "    :param game: the current game\n",
    "    :param agent: the agent who activates the method\n",
    "    :c the weight of the exploration part in the UCT\n",
    "    :d the weight of the heuristic\n",
    "    :return: void\n",
    "    \"\"\"\n",
    "\n",
    "    original_state = game.get_state()\n",
    "\n",
    "    # returns the reward if it's the end of the game, the selected action, and the given state after playing this action\n",
    "    reward, action_leaf, new_state = selection(root, game, c)\n",
    "\n",
    "    if not game.is_over():\n",
    "        action_leaf = expansion(action_leaf, new_state, game, agent.prune)\n",
    "\n",
    "        # adding the weighted heuristic value to the predicted reward from the DNN\n",
    "        reward = d * game.heuristic(new_state) + agent.model.forward(new_state)\n",
    "\n",
    "    back_propagation(action_leaf, reward)\n",
    "\n",
    "    # back to the original state of the game\n",
    "    game.set_state(original_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c122c6",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Parameters definition\n",
    "\"\"\"\n",
    "# training parms\n",
    "hp_model_training = dict(loss_fn=torch.nn.MSELoss(),\n",
    "                         batch_size=100,\n",
    "                         num_epochs=100,\n",
    "                         test_ratio=0.2,\n",
    "                         valid_ratio=0.2,\n",
    "                         early_stopping=100)\n",
    "# optimizer params\n",
    "hp_optimizer = dict(lr=0.001,\n",
    "                    weight_decay=0.01,\n",
    "                    momentum=0.99)\n",
    "# NN structure params\n",
    "hp_model = dict(hidden_layers_num=1,\n",
    "                hidden_layers_size=20,\n",
    "                activation='relu')\n",
    "\n",
    "#MCTS params: c - UCT exploration/exploitation param, d-weight on heuristic importance against the model(NN)\n",
    "hp_mcts = dict(c=1,\n",
    "               d=3,\n",
    "               iterations_num=200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec887d90",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# NN creation\n",
    "def create_model(in_dim, out_dim, model_file):\n",
    "    if os.path.isfile(model_file):\n",
    "        print(f'loading model from \"{model_file}\"...')\n",
    "        mlp = torch.load(model_file)\n",
    "        print(mlp)\n",
    "        return mlp\n",
    "\n",
    "    mlp = MLP(\n",
    "        in_dim=in_dim,\n",
    "        dims=[hp_model['hidden_layers_size']] * hp_model['hidden_layers_num'] + [out_dim],\n",
    "        nonlins=[hp_model['activation']] * hp_model['hidden_layers_num'] + ['none']\n",
    "    )\n",
    "\n",
    "    print('creating model...')\n",
    "    print(mlp)\n",
    "    return mlp\n",
    "\n",
    "\n",
    "# training function\n",
    "def train(dl_train, dl_valid, dl_test, model):\n",
    "    loss_fn = hp_model_training['loss_fn']\n",
    "    optimizer = torch.optim.SGD(params=model.parameters(), **hp_optimizer)\n",
    "    trainer = MLPTrainer(model, loss_fn, optimizer)\n",
    "\n",
    "    return trainer.fit(dl_train,\n",
    "                       dl_valid,\n",
    "                       num_epochs=hp_model_training['num_epochs'],\n",
    "                       print_every=10,\n",
    "                       early_stopping=hp_model_training['early_stopping'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "731644bc",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Agent holds the model it will be used to make decisions under the MCTS.\n",
    "Each agent can be use different model to predict the value function on a given state.\n",
    "\"\"\"\n",
    "class Agent:\n",
    "    def __init__(self, model, prune=True, rand=False):\n",
    "        self.model=model\n",
    "        self.prune=prune #boolean parameter, whether to prune or not\n",
    "        self.rand=rand #boolean parameter, whether to take an random action or nor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49cf6275",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def train_agent(games_num, model_path):\n",
    "    model = create_model(Catan.get_state_size(), Catan.get_players_num(), model_path)\n",
    "    agents = Catan.get_players_num()*[Agent(model)]\n",
    "\n",
    "    for i in range(1, games_num + 1):\n",
    "        print(f'_________________game {i}/{games_num}________________')\n",
    "\n",
    "        catan_game = Catan()\n",
    "\n",
    "        ds = Dataset(hp_model_training['batch_size'], hp_model_training['valid_ratio'], hp_model_training['test_ratio'])\n",
    "\n",
    "        turns_num = 0\n",
    "        actions_num = 0\n",
    "\n",
    "        while True:\n",
    "            actions_num += 1\n",
    "\n",
    "            best_action = mcts_get_best_action(catan_game, agents, hp_mcts['c'], hp_mcts['d'], hp_mcts['iterations_num'])\n",
    "            print(\"Player \" + str(catan_game.get_turn() + 1) + \", action:\" + str(best_action))\n",
    "\n",
    "            reward = catan_game.make_action(best_action)\n",
    "            if best_action[0] == 4:\n",
    "                turns_num += 1\n",
    "\n",
    "                print(\"Player \" + str(catan_game.get_turn() + 1) + \" turn!, dice: \" + str(catan_game.dice))\n",
    "                # print(catan_game.game.board)\n",
    "\n",
    "            ds.add_sample(catan_game.get_state())\n",
    "            if catan_game.is_over() or actions_num > 600:\n",
    "                if actions_num > 600:\n",
    "                  print(\"No winner, Final board:\")\n",
    "                  print(catan_game.game.board)\n",
    "                  reward = [catan_game.game.get_victory_points(catan_game.game.players[i]) for i in range (Catan.get_players_num())]\n",
    "                else:\n",
    "                  print(\"Congratulations! Player %d wins!\" % (catan_game.cur_id_player + 1))\n",
    "                  print(\"Final board:\")\n",
    "                  print(catan_game.game.board)\n",
    "\n",
    "\n",
    "                ds.set_label(reward)\n",
    "\n",
    "                dl_train, dl_valid, dl_test = ds.get_data_loaders()\n",
    "                fit_res = train(dl_train, dl_valid, dl_test, model)\n",
    "                plot_fit(fit_res, log_loss=False, train_test_overlay=True)\n",
    "                plt.show()\n",
    "                print(ds)\n",
    "\n",
    "                print(f'saving model in \"{model_path}\"')\n",
    "                torch.save(model, model_path)\n",
    "                break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54928215",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_agent(2, \"src/model2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee8cb42b",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Evaluation\n",
    "\n",
    "_define the evaluation metrics used to measure the sucess of your experiments. Explain why they are relevant for your problem_\n",
    "\n",
    "1. At the previous section we played number of games and trained our neural network. now we intend to run multiple games in order to collect statistics over agents' winning rate.\n",
    "2. The evaluation has been done by comparing different types of agents:<br>\n",
    "     a. $Agent 1$ - Trained NN with pruning.<br>\n",
    "     b. $Agent 2$ - Trained NN without pruning.<br>\n",
    "     c. $Agent 3$ - No NN with pruning.<br>\n",
    "     d. $Agent 4$ - No NN without pruning.<br>\n",
    "     c. $Agent 5$ - Random actions.<br>\n",
    "\n",
    "* Agents 1-4 is based on the MCTS, with 300 iterations, using heuristic.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeba6f8d",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def test_agent(games_num,agents):\n",
    "    stats = {k: [] for k in range(Catan.get_players_num())}\n",
    "    for i in range(1, games_num + 1):\n",
    "        print(f'_________________game {i}/{games_num}________________')\n",
    "        catan_game = Catan()\n",
    "        actions_num = 0 \n",
    "        turns_num = 0\n",
    "        while True:\n",
    "            actions_num += 1\n",
    "            if agents[catan_game.get_turn()].rand == True:\n",
    "                actions = catan_game.get_actions(prune=False)\n",
    "                best_action = random.choice(actions)\n",
    "                # reward = catan_game.make_action(tuple(best_action))\n",
    "            else:\n",
    "                best_action = mcts_get_best_action(catan_game, agents, hp_mcts['c'], hp_mcts['d'], hp_mcts['iterations_num'])\n",
    "\n",
    "            print(\"Player \" + str(catan_game.get_turn() + 1) + \", action:\" + str(best_action))\n",
    "            reward = catan_game.make_action(best_action)\n",
    "            if best_action[0] == 4:\n",
    "                turns_num += 1\n",
    "                print(\"Player \" + str(catan_game.get_turn() + 1) + \" turn!, dice: \" + str(catan_game.dice))\n",
    "                # print(catan_game.game.board)\n",
    "\n",
    "            if catan_game.is_over() or actions_num > 600:\n",
    "                if actions_num > 600:\n",
    "                  print(\"No winner, Final board:\")\n",
    "                  print(catan_game.game.board)\n",
    "                else:\n",
    "                  stats[catan_game.cur_id_player].append([actions_num, int(turns_num/4)])\n",
    "                  print(\"Congratulations! Player %d wins!\" % (catan_game.cur_id_player + 1))\n",
    "                  print(\"Final board:\")\n",
    "                  print(catan_game.game.board)\n",
    "                break\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af754b2",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Results\n",
    "\n",
    " _describe what you expected to see in your results._\n",
    " _describe your results and cmopare them to your expectations._\n",
    "\n",
    "1. We expected that the winning rate will be higher while using trained NN against non NN, and action pruning should be better than no pruning.\n",
    "2. We also expected that the NN should have greater impact over winning rate comparing the pruning.\n",
    "3. Obviously the random agent should perform the worst which was correctly anticipated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3347b8b3",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "num_of_tests = 4\n",
    "\n",
    "un_trained_model = create_model(Catan.get_state_size(), Catan.get_players_num(), 'model')\n",
    "trained_model = create_model(Catan.get_state_size(), Catan.get_players_num(), 'src/model2')\n",
    "agents = [Agent(trained_model), Agent(trained_model,prune=False), Agent(un_trained_model), Agent(un_trained_model,prune=False)]\n",
    "\n",
    "winning_records = test_agent(num_of_tests,agents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f3bc19",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "stats= [(100*len(v)/num_of_tests) for k,v in winning_records.items()]\n",
    "print(\"winning rate(prectnage):\")\n",
    "print(\"Agent 1 - Trained NN, with pruning:\" + str(stats[0]))\n",
    "print(\"Agent 2 - Trained NN, without pruning:\" + str(stats[1]))\n",
    "print(\"Agent 3 - No NN, with pruning:\" + str(stats[2]))\n",
    "print(\"Agent 4 - No NN, without pruning:\" + str(stats[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63866ba2",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Test random actions agent\n",
    "num_of_tests = 4\n",
    "\n",
    "un_trained_model = create_model(Catan.get_state_size(), Catan.get_players_num(), 'model')\n",
    "trained_model = create_model(Catan.get_state_size(), Catan.get_players_num(), 'src/model2')\n",
    "agents = [Agent(trained_model), Agent(trained_model,prune=False), Agent(un_trained_model,prune=False), Agent(un_trained_model,prune=False,rand=True)]\n",
    "\n",
    "winning_records = test_agent(num_of_tests,agents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8033f52b",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "winning rate(prectnage):\n",
      "Agent 1 - Trained NN, with pruning:25.0\n",
      "Agent 2 - Trained NN, without pruning:0.0\n",
      "Agent 4 - No NN, without pruning:0.0\n",
      "Agent 5 - Random actions:75.0\n"
     ]
    }
   ],
   "source": [
    "stats= [(100*len(v)/num_of_tests) for k,v in winning_records.items()]\n",
    "print(\"winning rate(prectnage):\")\n",
    "print(\"Agent 1 - Trained NN, with pruning:\" + str(stats[0]))\n",
    "print(\"Agent 2 - Trained NN, without pruning:\" + str(stats[1]))\n",
    "print(\"Agent 4 - No NN, without pruning:\" + str(stats[3]))\n",
    "print(\"Agent 5 - Random actions:\" + str(stats[2]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5012a12",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# TODO: visualize reults\n",
    "#   - tables of evaluation method\n",
    "#   - compare to previous works (if results available)\n",
    "#   - any other graphs / charts / plot that visualize your method's performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb1237d4",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Method Limitations / Possible Future Extensions\n",
    "\n",
    "_describe where any limitations or drawbacks to your method, as well as any suggestions for future work that will improve upon it._\n",
    "\n",
    "### Limitations\n",
    "1. Our method relay on fully state visibility (as in AlphaZero) because the DNN input has to be a game's state. For that reason we decided to simplify the game so that every state is fully visible to all player. The real game have more rules that damage the state visibility (As deveolpment cards, robber, map randomness and etc.) that we omitted. \n",
    "\n",
    "2. At the beginning, the DNN have random weights. Without any domain knowledge usage, the game episode may be infinite.\n",
    "\n",
    "3. Our algorithm demands high computational power which we cannot supply for both MCTS and DNN architecture. for that reason, we used a smaller amount of iterations(300) comparing to [Optimizing UCT for Settlers of Catan](https://www.sbgames.org/sbgames2017/papers/ComputacaoFull/175405.pdf) (10K) and we use a weaker NN architecture comparing to the mighty AlphaZero archietcture.\n",
    "\n",
    "###  Possible Future Extensions\n",
    "1. We can take inspration from [\"RE-L\"](https://cs230.stanford.edu/projects_fall_2021/reports/103176936.pdf) to use DQN in some parts of our algorithm, specifically at the initial stage picking we have a great impact over the entire game\n",
    "2. Adapting our algorithm to a POMDP( as in [\"blind-chess\"](https://towardsdatascience.com/blind-chess-log-0-d6b05c6cf90c) setting and use the original game rules. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d54cf1d4",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# TODO: show a running example that illustrates the above limitations\n",
    "# Use agents without heuristics\n",
    "hp_mcts['d']=0\n",
    "hp_mcts['iterations_num']=100\n",
    "model = create_model(Catan.get_state_size(), Catan.get_players_num(), \"new_model\")\n",
    "catan_game = Catan()\n",
    "agents = Catan.get_players_num()*[Agent(model, prune=False)]\n",
    "    \n",
    "for i in range(1000):\n",
    "    best_action = mcts_get_best_action(catan_game, agents, hp_mcts['c'], hp_mcts['d'], hp_mcts['iterations_num'])\n",
    "    reward = catan_game.make_action(best_action)\n",
    "\n",
    "    if catan_game.is_over(): \n",
    "        break\n",
    "        \n",
    "if catan_game.is_over():\n",
    "    print(\"Game is over after: \"+str(i)+\" actions\")\n",
    "else:\n",
    "    print(\"Game is going forever!\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "CatanNB.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}